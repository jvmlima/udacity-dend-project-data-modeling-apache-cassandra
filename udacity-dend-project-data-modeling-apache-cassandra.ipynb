{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/home\n"
     ]
    }
   ],
   "source": [
    "# checking your current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get your current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "# join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    #print(file_path_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8056\n",
      "['', 'Logged In', 'Adler', 'M', '0', 'Barrera', '', 'free', 'New York-Newark-Jersey City, NY-NJ-PA', 'GET', 'Home', '1.54084E+12', '248', '', '200', '1.54147E+12', '100']\n"
     ]
    }
   ],
   "source": [
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list \n",
    "for f in file_path_list:\n",
    "\n",
    "# reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    " # extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "# uncomment the code below if you would like to get total number of rows \n",
    "print(len(full_data_rows_list))\n",
    "# uncomment the code below if you would like to check to see what the list of event data rows will look like\n",
    "print(full_data_rows_list[0])\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_full csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin writing your Apache Cassandra code in the cells below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should make a connection to a Cassandra instance your local machine \n",
    "# (127.0.0.1)\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "\n",
    "# To establish connection and begin executing queries, need a session\n",
    "session = cluster.connect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS udacity \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.set_keyspace('udacity')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to create tables to run the following queries. Remember, with Apache Cassandra you model the database tables on the queries you want to run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create queries to ask the following three questions of the data\n",
    "\n",
    "### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "\n",
    "### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column types and the respective index in the .csv.\n",
    "# This data frame will help to generalize the creation, insert and validation of each table.\n",
    "\n",
    "df_cols = pd.DataFrame({'artist':        ['text',  0],\n",
    "                        'song':          ['text',  9], \n",
    "                        'length':        ['float', 5], \n",
    "                        'sessionid':     ['int',   8], \n",
    "                        'iteminsession': ['int',   3],\n",
    "                        'firstname':     ['text',  1], \n",
    "                        'lastname':      ['text',  4],  \n",
    "                        'userid':        ['int',   10]}).T\n",
    "\n",
    "df_cols.columns = ['dtype', 'line_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cols(df, include_type=False):\n",
    "    \"\"\" Parse the columns.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame)  : pandas dataframe whose index is the column name. It has a dtype column with the column type.\n",
    "        include_type (bool): bool to decide whether to include the column type in the concatenation. Default False.\n",
    "        \n",
    "    Return:\n",
    "        string of column names concatenated with a comma (can also include the column type).\n",
    "        \n",
    "    Example:\n",
    "    >>> parse_cols({'artist': 'text', 'length': 'float', include_type=True)\n",
    "    'artist text, length float'\n",
    "    \"\"\"\n",
    "    \n",
    "    sep = ', '\n",
    "    \n",
    "    if include_type:\n",
    "        parsed_cols = sep.join([df.index[i] + ' ' + df['dtype'][i] for i in range(len(df))])\n",
    "    else:\n",
    "        parsed_cols = sep.join([df.index[i]                        for i in range(len(df))])\n",
    "    \n",
    "    return parsed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_values(df,lines):\n",
    "    \"\"\"Return the line value converted to the correct data type.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame)  : pandas dataframe whose index is the column name. It has a dtype column with the column type.\n",
    "        lines (list): row values (read from .csv).\n",
    "        \n",
    "    Return:\n",
    "        tuple with converted values.\n",
    "    \"\"\"\n",
    "    funs = {'text': str, 'float': float, 'int': int}\n",
    "    line_values = []\n",
    "    for i in range(len(df)):\n",
    "        dtype = df['dtype'][i]\n",
    "        line_index = df['line_index'][i]\n",
    "        line_value = funs[dtype](lines[line_index])\n",
    "        line_values.append(line_value)\n",
    "    return tuple(line_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(tbl,cols_create,cols_pk):\n",
    "    \"\"\"Create a table in apache cassandra.\n",
    "    \"\"\"\n",
    "    \n",
    "    cols_pk_parsed     = parse_cols(df_cols.loc[cols_pk],    include_type=False)\n",
    "    cols_create_parsed = parse_cols(df_cols.loc[cols_create],include_type=True)\n",
    "    \n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {tbl} \"\n",
    "    query = query + f\"({cols_create_parsed}, PRIMARY KEY ({cols_pk_parsed}))\"\n",
    "\n",
    "    try:\n",
    "        session.execute(query)\n",
    "    except Exception as e:\n",
    "        print(e)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(tbl, df_cols, cols_create):\n",
    "    \"\"\"Insert data into a table in apache cassandra.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = 'event_datafile_new.csv'\n",
    "\n",
    "    cols_insert_parsed = parse_cols(df_cols.loc[cols_create],include_type=False)\n",
    "    insert_placeholder = ', '.join(['%s'] * len(cols_create))\n",
    "    \n",
    "    with open(file, encoding = 'utf8') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        next(csvreader) # skip header\n",
    "        for line in csvreader:\n",
    "            line_values = get_line_values(df_cols.loc[cols_create],line)\n",
    "            query = f\"INSERT INTO {tbl} ({cols_insert_parsed})\"\n",
    "            query = query + f\" VALUES ({insert_placeholder})\"\n",
    "            session.execute(query, line_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def verify_data(tbl, df_cols, cols_select, where_statement):\n",
    "    \"\"\"Prints the table rows to verify the content.\n",
    "    \"\"\"\n",
    "    \n",
    "    cols_select_parsed = parse_cols(df_cols.loc[cols_select],include_type=False)\n",
    "    \n",
    "    query = f\"select {cols_select_parsed} from {tbl} where {where_statement}\"\n",
    "    \n",
    "    try:\n",
    "        rows = session.execute(query)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    t = PrettyTable(cols_select)\n",
    "    for row in rows:\n",
    "        row_content = []\n",
    "        for i,attr in enumerate(cols_select):\n",
    "            row_content.append(row[i])\n",
    "        t.add_row(row_content)\n",
    "    print(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this query we need the information about the sessions and the songs listened, so the table that will be build will be composed of the following columns:\n",
    "- Partition key: `sessionid`\n",
    "- Clustering columns: `iteminsession`\n",
    "- Selected columns: `artist`, `song`, `length`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Give me the artist, song title and song's length in the music app history that was heard \n",
    "# during sessionId = 338, and itemInSession = 4\n",
    "\n",
    "tbl_query_1     = 'tbl_song_details'\n",
    "\n",
    "cols_partkey    = ['sessionid']\n",
    "cols_cluster    = ['iteminsession']\n",
    "cols_select     = ['artist', 'song', 'length']\n",
    "cols_primkey    = cols_partkey + cols_cluster\n",
    "cols_create     = cols_primkey + cols_select\n",
    "# cols_create     = ['artist', 'song', 'length', 'sessionid', 'iteminsession']\n",
    "\n",
    "where_statement = 'sessionid=338 and iteminsession=4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(tbl_query_1, cols_create, cols_primkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data(tbl_query_1, df_cols, cols_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+--------------------+\n",
      "|   artist  |               song              |       length       |\n",
      "+-----------+---------------------------------+--------------------+\n",
      "| Faithless | Music Matters (Mark Knight Dub) | 495.30731201171875 |\n",
      "+-----------+---------------------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "verify_data(tbl_query_1, df_cols, cols_select, where_statement)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this query we need the information about the sessions, songs and users, so the table that will be build will be composed of the following columns:\n",
    "- Partition key: `userid`, `sessionid`\n",
    "- Clustering columns: `iteminsession`\n",
    "- Selected columns: `artist`, `song`, `firstname`, `lastname`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name)\n",
    "# for userid = 10, sessionid = 182\n",
    "\n",
    "tbl_query_2     = 'tbl_artist_details'\n",
    "\n",
    "cols_partkey    = ['userid', 'sessionid']\n",
    "cols_cluster    = ['iteminsession']\n",
    "cols_select     = ['artist', 'song', 'firstname', 'lastname']\n",
    "cols_primkey    = cols_partkey + cols_cluster\n",
    "cols_create     = cols_primkey + cols_select\n",
    "\n",
    "where_statement = 'userid = 10 and sessionid = 182'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(tbl_query_2, cols_create, cols_primkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data(tbl_query_2, df_cols, cols_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------------------------------------+-----------+----------+\n",
      "|       artist      |                         song                         | firstname | lastname |\n",
      "+-------------------+------------------------------------------------------+-----------+----------+\n",
      "|  Down To The Bone |                  Keep On Keepin' On                  |   Sylvie  |   Cruz   |\n",
      "|    Three Drives   |                     Greece 2000                      |   Sylvie  |   Cruz   |\n",
      "| Sebastien Tellier |                      Kilometer                       |   Sylvie  |   Cruz   |\n",
      "|   Lonnie Gordon   | Catch You Baby (Steve Pitron & Max Sanna Radio Edit) |   Sylvie  |   Cruz   |\n",
      "+-------------------+------------------------------------------------------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "verify_data(tbl_query_2, df_cols, cols_select, where_statement)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this query we need the information about the songs and users, so the table that will be build will be composed of the following columns:\n",
    "- Partition key: `song`\n",
    "- Clustering columns: `userid`\n",
    "- Selected columns: `firstname`, `lastname`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Give me every user name (first and last) in my music app history who listened to \n",
    "# the song 'All Hands Against His Own'\n",
    "\n",
    "tbl_query_3     = 'tbl_user_details'\n",
    "\n",
    "cols_partkey    = ['song']\n",
    "cols_cluster    = ['userid']\n",
    "cols_select     = ['firstname', 'lastname']\n",
    "cols_primkey    = cols_partkey + cols_cluster\n",
    "cols_create     = cols_primkey + cols_select\n",
    "\n",
    "where_statement = \"song = 'All Hands Against His Own'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(tbl_query_3, cols_create, cols_primkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data(tbl_query_3, df_cols, cols_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "| firstname  | lastname |\n",
      "+------------+----------+\n",
      "| Jacqueline |  Lynch   |\n",
      "|   Tegan    |  Levine  |\n",
      "|    Sara    | Johnson  |\n",
      "+------------+----------+\n"
     ]
    }
   ],
   "source": [
    "verify_data(tbl_query_3, df_cols, cols_select, where_statement)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbls = [tbl_query_1, tbl_query_2, tbl_query_3]\n",
    "\n",
    "for tbl in tbls:\n",
    "    query = f\"drop table {tbl}\"\n",
    "    try:\n",
    "        rows = session.execute(query)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
